{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open StreetMap Data Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map Area\n",
    "### Nashville, TN, United States\n",
    "\n",
    "#### https://www.openstreetmap.org/#map=13/36.1154/-86.8430\n",
    "\n",
    "##### I am interested in wrangling and cleaning OpenStreetMap data. Nashville, TN is my hometown and this neighborhood is of a particular interest to me. I explored the dataset, cleaned the XML data, exported it to a CSV file and loaded it into a SQL database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I parsed through the Nashville dataset with ElementTree to discover the number of unique element types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'node': 226056, 'member': 19394, 'nd': 283605, 'tag': 116507, 'bounds': 1, 'note': 1, 'meta': 1, 'relation': 453, 'way': 32604, 'osm': 1}\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "\n",
    "dataset = \"map.osm\"\n",
    "\n",
    "def count_tags(filename):\n",
    "    tag_dict = {}\n",
    "    for event, elem in ET.iterparse(filename):\n",
    "        if elem.tag not in tag_dict:\n",
    "            tag_dict[elem.tag] = 1\n",
    "        else:\n",
    "            tag_dict[elem.tag] += 1\n",
    "\n",
    "    return tag_dict\n",
    "\n",
    "print(count_tags(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems with the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The data we acquired the OpenStreetMap is crowd-sourced, so there are some inconsistencies we need to account for. All elements won't share the same formatting (e.g. upper-case, lower-case) and some characters will cause problems with code syntax (e.g. colons). Here, I compare elements to a dictionary of keys. If the element is tagged, then its attribute 'k' is analyzed to look for these problem characters. It will then show us how many potential issues will have in each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'problemchars': 1, 'lower': 82683, 'other': 2148, 'lower_colon': 31675}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "\n",
    "def key_type(element, keys):\n",
    "    \"\"\"\n",
    "    Takes in an XML element (<node />, <way />) and a dictionary of keys.\n",
    "    If the element is a <tag />, its ['k'] attribute is analyzed,\n",
    "    the dictionary keys and lists are incremented accordingly,\n",
    "    - lower: valid tags that only contain lowercase letters\n",
    "    - lower_colon: valid tags that contain lowercase letter with one or more colons in their name\n",
    "    - problemchars: tags with problematic characters\n",
    "    - other: tags that don't fall in the previous three categories\n",
    "    \"\"\"\n",
    "    \n",
    "    if element.tag == 'tag':\n",
    "        if lower.search(element.attrib['k']):\n",
    "            keys['lower'] +=1\n",
    "        elif lower_colon.search(element.attrib['k']):\n",
    "            keys['lower_colon'] += 1\n",
    "        elif problemchars.search(element.attrib['k']):\n",
    "            keys['problemchars'] += 1\n",
    "        else:\n",
    "            keys['other'] += 1\n",
    "    return keys\n",
    "\n",
    "\n",
    "def process_tags(filename):\n",
    "    \"\"\"\n",
    "    Takes in a dataset in XML format, parses it, and executes the function key_type() for each element.\n",
    "    Returns a dictionary with the count of different types of keys.\n",
    "    \"\"\"\n",
    "    keys = {\"lower\": 0, \"lower_colon\": 0, \"problemchars\": 0, \"other\": 0}\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        keys = key_type(element, keys)\n",
    "\n",
    "    return keys\n",
    "\n",
    "tag_type_dict = process_tags(dataset)\n",
    "print (tag_type_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There is one potential tag that has problematic characters. Below is a function that will return our problematic key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Stahlman']\n"
     ]
    }
   ],
   "source": [
    "def get_problemkeys(filename):\n",
    "    problemchars_list = []\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        if element.tag == 'tag':\n",
    "            if problemchars.search(element.attrib['k']):\n",
    "                problemchars_list.append(element.attrib['k'])\n",
    "    return problemchars_list\n",
    "\n",
    "print(get_problemkeys(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No concerns with one space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auditing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, let's take a look at the data to see what unique street types it contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(defaultdict(<type 'int'>, {'1305': 1, 'Boulevard': 10, 'Court': 15, 'West': 1, 'Rd': 2, 'Way': 1, '#118': 1, 'BLVD': 2, 'ave': 1, 'East': 1, 'AVENUE': 3, 'Davidson': 1, 'North': 71, 'avenue': 1, 'E106': 1, 'Vandy': 1, 'South': 131, 'A': 1, 'Pike': 27, 'Lane': 33, 'Drive': 36, 'St': 1, '263': 1, 'S': 3, 'Place': 23, 'Ave': 1, 'Dea': 1, 'Parkway': 4, 'N': 3, 'Road': 11, 'Blvd.': 1, 'Nolensville': 1, 'Alley': 4, 'Street': 164, 'Blvd': 1, 'Broadway': 36, 'Avenue': 128}), '%s: %d')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'#118': 1,\n",
       "             '1305': 1,\n",
       "             '263': 1,\n",
       "             'A': 1,\n",
       "             'AVENUE': 3,\n",
       "             'Alley': 4,\n",
       "             'Ave': 1,\n",
       "             'Avenue': 128,\n",
       "             'BLVD': 2,\n",
       "             'Blvd': 1,\n",
       "             'Blvd.': 1,\n",
       "             'Boulevard': 10,\n",
       "             'Broadway': 36,\n",
       "             'Court': 15,\n",
       "             'Davidson': 1,\n",
       "             'Dea': 1,\n",
       "             'Drive': 36,\n",
       "             'E106': 1,\n",
       "             'East': 1,\n",
       "             'Lane': 33,\n",
       "             'N': 3,\n",
       "             'Nolensville': 1,\n",
       "             'North': 71,\n",
       "             'Parkway': 4,\n",
       "             'Pike': 27,\n",
       "             'Place': 23,\n",
       "             'Rd': 2,\n",
       "             'Road': 11,\n",
       "             'S': 3,\n",
       "             'South': 131,\n",
       "             'St': 1,\n",
       "             'Street': 164,\n",
       "             'Vandy': 1,\n",
       "             'Way': 1,\n",
       "             'West': 1,\n",
       "             'ave': 1,\n",
       "             'avenue': 1})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "street_type_re = re.compile(r'\\S+\\.?$', re.IGNORECASE)\n",
    "street_types = defaultdict(int)\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        street_types[street_type] += 1\n",
    "\n",
    "def print_sorted_dict(d, expression):\n",
    "    keys = d.keys()\n",
    "    keys = sorted(keys, key=lambda s: s.lower())\n",
    "    for k in keys:\n",
    "        v = d[k]\n",
    "        print (expression % (k, v))\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.tag == \"tag\") and (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "def audit(filename):\n",
    "    for event, elem in ET.iterparse(filename):\n",
    "        if is_street_name(elem):\n",
    "            audit_street_type(street_types, elem.attrib['v'])\n",
    "    print(street_types, \"%s: %d\")\n",
    "    return(street_types)\n",
    "\n",
    "all_types = audit(dataset)\n",
    "all_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Items to Address\n",
    "- Abbreviations\n",
    "- Suites\n",
    "- Misspelled street names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addressing Abbreviations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below is the definition of a dictionary of valid street types and corrections to implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected = ['Artery', 'Alley', 'Avenue', 'Boulevard', 'Broadway', 'Commons', 'Court', 'Drive', 'Lane', 'Park', 'Parkway',\n",
    "            'Place', 'Road', 'Square', 'Street', 'Terrace', 'Trail', 'Turnpike', 'Wharf',\n",
    "            'Yard']\n",
    "\n",
    "abbr_mapping = { 'Ave': 'Avenue',\n",
    "                  'Ave.': 'Avenue',\n",
    "                  'AVENUE': 'Avenue',\n",
    "                  'ave': 'Avenue',\n",
    "                  'avenue': 'Avenue',\n",
    "                  'St': 'Street',\n",
    "                  'BLVD': 'Boulevard',\n",
    "                  'Blvd': 'Boulevard',\n",
    "                  'Blvd.': 'Boulevard',\n",
    "                  'Rd': 'Road',\n",
    "                  'S': 'South',                  \n",
    "                  'N': 'North'\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below, i will print some of the problematic names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'#118': ['4th Ave S #118'],\n",
       " '1305': ['1305'],\n",
       " '263': ['Houston Street, Suite 263'],\n",
       " 'A': ['Broadway A'],\n",
       " 'Davidson': ['Davidson'],\n",
       " 'Dea': ['Dea'],\n",
       " 'E106': ['Seaboard Ln Ste E106'],\n",
       " 'East': ['Music Square East'],\n",
       " 'Nolensville': ['Nolensville'],\n",
       " 'Vandy': ['Village at Vandy'],\n",
       " 'Way': [\"Children's Way\"],\n",
       " 'West': ['Music Square West']}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "typo_full_names = {}\n",
    "\n",
    "def audit_street_name(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if (all_types[street_type] < 20) and (street_type not in expected) and (street_type not in abbr_mapping):\n",
    "            if street_type in typo_full_names:\n",
    "                typo_full_names[street_type].append(street_name)\n",
    "            else:\n",
    "                typo_full_names.update({ street_type:[street_name] })\n",
    "\n",
    "def audit_name(filename):\n",
    "    for event, elem in ET.iterparse(filename):\n",
    "        if is_street_name(elem):\n",
    "            audit_street_name(street_types, elem.attrib['v'])    \n",
    "    # print_sorted_dict(street_types)\n",
    "    return typo_full_names\n",
    "\n",
    "audit_name(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Way needs to be added to the dictionary of expected street endings.\n",
    "- Suite numbers need to be addressed.\n",
    "- Davidson is the name of a street.\n",
    "- 'Dea' is an abbreviation for Deadrick Street\n",
    "- 'Village at Vandy' is on the Vanderbilt campus\n",
    "- Music Square East and West are actual street names\n",
    "- Nolensville should be Nolensville Pike"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addressing Suites and Other Updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Alley',\n",
       " 'Alley',\n",
       " 'Artery',\n",
       " 'Avenue',\n",
       " 'Boulevard',\n",
       " 'Broadway',\n",
       " 'Commons',\n",
       " 'Court',\n",
       " 'Drive',\n",
       " 'East',\n",
       " 'Lane',\n",
       " 'Park',\n",
       " 'Parkway',\n",
       " 'Place',\n",
       " 'Road',\n",
       " 'Square',\n",
       " 'Street',\n",
       " 'Terrace',\n",
       " 'Trail',\n",
       " 'Turnpike',\n",
       " 'Way',\n",
       " 'West',\n",
       " 'Wharf',\n",
       " 'Yard']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected.extend(['Alley', 'West', 'East', 'Way'])\n",
    "\n",
    "typo_mapping = { 'Broadway A': 'Broadway',\n",
    "                 'Dea': 'Deaderick Street',\n",
    "                 'Davidson': 'Davidson Street',\n",
    "                 'Seaboard Ln Ste E106': {'Seaboard Lane': 'Suite E106'},\n",
    "                 'Village at Vandy': 'Village at Vandy Square',\n",
    "                 'Nolensville': 'Nolensville Pike',\n",
    "                 \"Children's Way\": 'Childrens Way'\n",
    "                \n",
    "               }\n",
    "\n",
    "numbers_mapping = {'#118': {'4th Avenue South': 'Suite 118'},\n",
    "                    '1305': {''},\n",
    "                    '263': {'Houston Street': 'Suite 263'}\n",
    "                  }\n",
    "\n",
    "expected = sorted(expected)\n",
    "expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below are street names that may contain problematic characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Children's Way\", '4th Ave S #118', 'Houston Street, Suite 263']\n"
     ]
    }
   ],
   "source": [
    "name_problem_chars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\t\\r\\n]')\n",
    "\n",
    "def get_problem_names(filename):\n",
    "    problemchars_list = []\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        if is_street_name(element):\n",
    "            if name_problem_chars.search(element.attrib['v']):\n",
    "                problemchars_list.append(element.attrib['v'])\n",
    "    return problemchars_list\n",
    "\n",
    "print(get_problem_names(dataset))\n",
    "problemchars_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the provisional data.py file to parse and create csv files for our SQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "After auditing is complete the next step is to prepare the data to be inserted into a SQL database.\n",
    "To do so you will parse the elements in the OSM XML file, transforming them from document format to\n",
    "tabular format, thus making it possible to write to .csv files.  These csv files can then easily be\n",
    "imported to a SQL database as tables.\n",
    "\n",
    "The process for this transformation is as follows:\n",
    "- Use iterparse to iteratively step through each top level element in the XML\n",
    "- Shape each element into several data structures using a custom function\n",
    "- Utilize a schema and validation library to ensure the transformed data is in the correct format\n",
    "- Write each data structure to the appropriate .csv files\n",
    "\n",
    "We've already provided the code needed to load the data, perform iterative parsing and write the\n",
    "output to csv files. Your task is to complete the shape_element function that will transform each\n",
    "element into the correct format. To make this process easier we've already defined a schema (see\n",
    "the schema.py file in the last code tab) for the .csv files and the eventual tables. Using the \n",
    "cerberus library we can validate the output against this schema to ensure it is correct.\n",
    "\n",
    "## Shape Element Function\n",
    "The function should take as input an iterparse Element object and return a dictionary.\n",
    "\n",
    "### If the element top level tag is \"node\":\n",
    "The dictionary returned should have the format {\"node\": .., \"node_tags\": ...}\n",
    "\n",
    "The \"node\" field should hold a dictionary of the following top level node attributes:\n",
    "- id\n",
    "- user\n",
    "- uid\n",
    "- version\n",
    "- lat\n",
    "- lon\n",
    "- timestamp\n",
    "- changeset\n",
    "All other attributes can be ignored\n",
    "\n",
    "The \"node_tags\" field should hold a list of dictionaries, one per secondary tag. Secondary tags are\n",
    "child tags of node which have the tag name/type: \"tag\". Each dictionary should have the following\n",
    "fields from the secondary tag attributes:\n",
    "- id: the top level node id attribute value\n",
    "- key: the full tag \"k\" attribute value if no colon is present or the characters after the colon if one is.\n",
    "- value: the tag \"v\" attribute value\n",
    "- type: either the characters before the colon in the tag \"k\" value or \"regular\" if a colon\n",
    "        is not present.\n",
    "\n",
    "Additionally,\n",
    "\n",
    "- if the tag \"k\" value contains problematic characters, the tag should be ignored\n",
    "- if the tag \"k\" value contains a \":\" the characters before the \":\" should be set as the tag type\n",
    "  and characters after the \":\" should be set as the tag key\n",
    "- if there are additional \":\" in the \"k\" value they and they should be ignored and kept as part of\n",
    "  the tag key. For example:\n",
    "\n",
    "  <tag k=\"addr:street:name\" v=\"Lincoln\"/>\n",
    "  should be turned into\n",
    "  {'id': 12345, 'key': 'street:name', 'value': 'Lincoln', 'type': 'addr'}\n",
    "\n",
    "- If a node has no secondary tags then the \"node_tags\" field should just contain an empty list.\n",
    "\n",
    "The final return value for a \"node\" element should look something like:\n",
    "\n",
    "{'node': {'id': 757860928,\n",
    "          'user': 'uboot',\n",
    "          'uid': 26299,\n",
    "       'version': '2',\n",
    "          'lat': 41.9747374,\n",
    "          'lon': -87.6920102,\n",
    "          'timestamp': '2010-07-22T16:16:51Z',\n",
    "      'changeset': 5288876},\n",
    " 'node_tags': [{'id': 757860928,\n",
    "                'key': 'amenity',\n",
    "                'value': 'fast_food',\n",
    "                'type': 'regular'},\n",
    "               {'id': 757860928,\n",
    "                'key': 'cuisine',\n",
    "                'value': 'sausage',\n",
    "                'type': 'regular'},\n",
    "               {'id': 757860928,\n",
    "                'key': 'name',\n",
    "                'value': \"Shelly's Tasty Freeze\",\n",
    "                'type': 'regular'}]}\n",
    "\n",
    "### If the element top level tag is \"way\":\n",
    "The dictionary should have the format {\"way\": ..., \"way_tags\": ..., \"way_nodes\": ...}\n",
    "\n",
    "The \"way\" field should hold a dictionary of the following top level way attributes:\n",
    "- id\n",
    "-  user\n",
    "- uid\n",
    "- version\n",
    "- timestamp\n",
    "- changeset\n",
    "\n",
    "All other attributes can be ignored\n",
    "\n",
    "The \"way_tags\" field should again hold a list of dictionaries, following the exact same rules as\n",
    "for \"node_tags\".\n",
    "\n",
    "Additionally, the dictionary should have a field \"way_nodes\". \"way_nodes\" should hold a list of\n",
    "dictionaries, one for each nd child tag.  Each dictionary should have the fields:\n",
    "- id: the top level element (way) id\n",
    "- node_id: the ref attribute value of the nd tag\n",
    "- position: the index starting at 0 of the nd tag i.e. what order the nd tag appears within\n",
    "            the way element\n",
    "\n",
    "The final return value for a \"way\" element should look something like:\n",
    "\n",
    "{'way': {'id': 209809850,\n",
    "         'user': 'chicago-buildings',\n",
    "         'uid': 674454,\n",
    "         'version': '1',\n",
    "         'timestamp': '2013-03-13T15:58:04Z',\n",
    "         'changeset': 15353317},\n",
    " 'way_nodes': [{'id': 209809850, 'node_id': 2199822281, 'position': 0},\n",
    "               {'id': 209809850, 'node_id': 2199822390, 'position': 1},\n",
    "               {'id': 209809850, 'node_id': 2199822392, 'position': 2},\n",
    "               {'id': 209809850, 'node_id': 2199822369, 'position': 3},\n",
    "               {'id': 209809850, 'node_id': 2199822370, 'position': 4},\n",
    "               {'id': 209809850, 'node_id': 2199822284, 'position': 5},\n",
    "               {'id': 209809850, 'node_id': 2199822281, 'position': 6}],\n",
    " 'way_tags': [{'id': 209809850,\n",
    "               'key': 'housenumber',\n",
    "               'type': 'addr',\n",
    "               'value': '1412'},\n",
    "              {'id': 209809850,\n",
    "               'key': 'street',\n",
    "               'type': 'addr',\n",
    "               'value': 'West Lexington St.'},\n",
    "              {'id': 209809850,\n",
    "               'key': 'street:name',\n",
    "               'type': 'addr',\n",
    "               'value': 'Lexington'},\n",
    "              {'id': '209809850',\n",
    "               'key': 'street:prefix',\n",
    "               'type': 'addr',\n",
    "               'value': 'West'},\n",
    "              {'id': 209809850,\n",
    "               'key': 'street:type',\n",
    "               'type': 'addr',\n",
    "               'value': 'Street'},\n",
    "              {'id': 209809850,\n",
    "               'key': 'building',\n",
    "               'type': 'regular',\n",
    "               'value': 'yes'},\n",
    "              {'id': 209809850,\n",
    "               'key': 'levels',\n",
    "               'type': 'building',\n",
    "               'value': '1'},\n",
    "              {'id': 209809850,\n",
    "               'key': 'building_id',\n",
    "               'type': 'chicago',\n",
    "               'value': '366409'}]}\n",
    "\"\"\"\n",
    "\n",
    "import csv\n",
    "import codecs\n",
    "import pprint\n",
    "import re\n",
    "import xml.etree.cElementTree as ET\n",
    "\n",
    "import cerberus\n",
    "\n",
    "import schema\n",
    "\n",
    "OSM_PATH = \"map.osm\"\n",
    "\n",
    "NODES_PATH = \"nodes.csv\"\n",
    "NODE_TAGS_PATH = \"nodes_tags.csv\"\n",
    "WAYS_PATH = \"ways.csv\"\n",
    "WAY_NODES_PATH = \"ways_nodes.csv\"\n",
    "WAY_TAGS_PATH = \"ways_tags.csv\"\n",
    "\n",
    "LOWER_COLON = re.compile(r'^([a-z]|_)+:([a-z]|_)+')\n",
    "PROBLEMCHARS = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "SCHEMA = schema.schema\n",
    "\n",
    "# Make sure the fields order in the csvs matches the column order in the sql table schema\n",
    "NODE_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_NODES_FIELDS = ['id', 'node_id', 'position']\n",
    "\n",
    "def shape_element(element, node_attr_fields=NODE_FIELDS, way_attr_fields=WAY_FIELDS,\n",
    "                  problem_chars=PROBLEMCHARS, default_tag_type='regular'):\n",
    "    \"\"\"Clean and shape node or way XML element to Python dict\"\"\"\n",
    "\n",
    "    node_attribs = {}\n",
    "    way_attribs = {}\n",
    "    way_nodes = []\n",
    "    tags = []  # Handle secondary tags the same way for both node and way elements\n",
    "\n",
    "    if element.tag == 'node':\n",
    "        node_attribs['id'] = element.attrib['id']\n",
    "        node_attribs['user'] = element.attrib['user']\n",
    "        node_attribs['uid'] = element.attrib['uid']\n",
    "        node_attribs['version'] = element.attrib['version']\n",
    "        node_attribs['lat'] = element.attrib['lat']\n",
    "        node_attribs['lon'] = element.attrib['lon']\n",
    "        node_attribs['timestamp'] = element.attrib['timestamp']\n",
    "        node_attribs['changeset'] = element.attrib['changeset']\n",
    "        \n",
    "        for node in element:\n",
    "            tag_dict = {}\n",
    "            tag_dict['id'] = element.attrib['id']\n",
    "            if ':' in node.attrib['k']:\n",
    "                tag_dict['type'] = node.attrib['k'].split(':', 1)[0]\n",
    "                tag_dict['key'] = node.attrib['k'].split(':', 1)[-1]\n",
    "                tag_dict['value'] = node.attrib['v'].split(':', 1)[0]\n",
    "            else:\n",
    "                tag_dict['type'] = 'regular'\n",
    "                tag_dict['key'] = node.attrib['k']\n",
    "                tag_dict['value'] = node.attrib['v']\n",
    "            tags.append(tag_dict)\n",
    "            \n",
    "    elif element.tag == 'way':\n",
    "        way_attribs['id'] = element.attrib['id']\n",
    "        way_attribs['user'] = element.attrib['user']\n",
    "        way_attribs['uid'] = element.attrib['uid']\n",
    "        way_attribs['version'] = element.attrib['version']\n",
    "        way_attribs['timestamp'] = element.attrib['timestamp']\n",
    "        way_attribs['changeset'] = element.attrib['changeset']\n",
    "        n = 0\n",
    "        for node in element:\n",
    "            if node.tag == 'nd':\n",
    "                way_dict = {}\n",
    "                way_dict['id'] = element.attrib['id']\n",
    "                way_dict['node_id'] = node.attrib['ref']\n",
    "                way_dict['position'] = n\n",
    "                n += 1\n",
    "                way_nodes.append(way_dict)\n",
    "            if node.tag == 'tag':\n",
    "                tag_dict = {}\n",
    "                tag_dict['id'] = element.attrib['id']\n",
    "                if ':' in node.attrib['k']:\n",
    "                    tag_dict['type'] = node.attrib['k'].split(':', 1)[0]\n",
    "                    tag_dict['key'] = node.attrib['k'].split(':', 1)[-1]\n",
    "                    tag_dict['value'] = node.attrib['v'].split(':', 1)[0]\n",
    "                else:\n",
    "                    tag_dict['type'] = 'regular'\n",
    "                    tag_dict['key'] = node.attrib['k']\n",
    "                    tag_dict['value'] = node.attrib['v']\n",
    "                tags.append(tag_dict)\n",
    "\n",
    "    if element.tag == 'node':\n",
    "        return {'node': node_attribs, 'node_tags': tags}\n",
    "    elif element.tag == 'way':\n",
    "        return {'way': way_attribs, 'way_nodes': way_nodes, 'way_tags': tags}\n",
    "\n",
    "\n",
    "# ================================================== #\n",
    "#               Helper Functions                     #\n",
    "# ================================================== #\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\"\"\"\n",
    "\n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "def validate_element(element, validator, schema=SCHEMA):\n",
    "    \"\"\"Raise ValidationError if element does not match schema\"\"\"\n",
    "    if validator.validate(element, schema) is not True:\n",
    "        field, errors = next(validator.errors.iteritems())\n",
    "        message_string = \"\\nElement of type '{0}' has the following errors:\\n{1}\"\n",
    "        error_string = pprint.pformat(errors)\n",
    "        \n",
    "        raise Exception(message_string.format(field, error_string))\n",
    "\n",
    "\n",
    "class UnicodeDictWriter(csv.DictWriter, object):\n",
    "    \"\"\"Extend csv.DictWriter to handle Unicode input\"\"\"\n",
    "\n",
    "    def writerow(self, row):\n",
    "        super(UnicodeDictWriter, self).writerow({\n",
    "            k: (v.encode('utf-8') if isinstance(v, unicode) else v) for k, v in row.iteritems()\n",
    "        })\n",
    "\n",
    "    def writerows(self, rows):\n",
    "        for row in rows:\n",
    "            self.writerow(row)\n",
    "\n",
    "\n",
    "# ================================================== #\n",
    "#               Main Function                        #\n",
    "# ================================================== #\n",
    "def process_map(file_in, validate):\n",
    "    \"\"\"Iteratively process each XML element and write to csv(s)\"\"\"\n",
    "\n",
    "    with codecs.open(NODES_PATH, 'w') as nodes_file, \\\n",
    "         codecs.open(NODE_TAGS_PATH, 'w') as nodes_tags_file, \\\n",
    "         codecs.open(WAYS_PATH, 'w') as ways_file, \\\n",
    "         codecs.open(WAY_NODES_PATH, 'w') as way_nodes_file, \\\n",
    "         codecs.open(WAY_TAGS_PATH, 'w') as way_tags_file:\n",
    "\n",
    "        nodes_writer = UnicodeDictWriter(nodes_file, NODE_FIELDS)\n",
    "        node_tags_writer = UnicodeDictWriter(nodes_tags_file, NODE_TAGS_FIELDS)\n",
    "        ways_writer = UnicodeDictWriter(ways_file, WAY_FIELDS)\n",
    "        way_nodes_writer = UnicodeDictWriter(way_nodes_file, WAY_NODES_FIELDS)\n",
    "        way_tags_writer = UnicodeDictWriter(way_tags_file, WAY_TAGS_FIELDS)\n",
    "\n",
    "        nodes_writer.writeheader()\n",
    "        node_tags_writer.writeheader()\n",
    "        ways_writer.writeheader()\n",
    "        way_nodes_writer.writeheader()\n",
    "        way_tags_writer.writeheader()\n",
    "\n",
    "        validator = cerberus.Validator()\n",
    "\n",
    "        for element in get_element(file_in, tags=('node', 'way')):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                if validate is True:\n",
    "                    validate_element(el, validator)\n",
    "\n",
    "                if element.tag == 'node':\n",
    "                    nodes_writer.writerow(el['node'])\n",
    "                    node_tags_writer.writerows(el['node_tags'])\n",
    "                elif element.tag == 'way':\n",
    "                    ways_writer.writerow(el['way'])\n",
    "                    way_nodes_writer.writerows(el['way_nodes'])\n",
    "                    way_tags_writer.writerows(el['way_tags'])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Note: Validation is ~ 10X slower. For the project consider using a small\n",
    "    # sample of the map when validating.\n",
    "    process_map(OSM_PATH, validate=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Creating database on disk\n",
    "sqlite_file = 'nashville.db'\n",
    "conn = sqlite3.connect(sqlite_file)\n",
    "conn.text_factory = str\n",
    "c = conn.cursor()\n",
    "\n",
    "c.execute('''DROP TABLE IF EXISTS nodes''')\n",
    "c.execute('''DROP TABLE IF EXISTS nodes_tags''')\n",
    "c.execute('''DROP TABLE IF EXISTS ways''')\n",
    "c.execute('''DROP TABLE IF EXISTS ways_tags''')\n",
    "c.execute('''DROP TABLE IF EXISTS ways_nodes''')\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_NODES = \"\"\"\n",
    "CREATE TABLE nodes (\n",
    "    id INTEGER NOT NULL,\n",
    "    lat REAL,\n",
    "    lon REAL,\n",
    "    user TEXT,\n",
    "    uid INTEGER,\n",
    "    version INTEGER,\n",
    "    changeset INTEGER,\n",
    "    timestamp TEXT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "QUERY_NODES_TAGS = \"\"\"\n",
    "CREATE TABLE nodes_tags (\n",
    "    id INTEGER,\n",
    "    key TEXT,\n",
    "    value TEXT,\n",
    "    type TEXT,\n",
    "    FOREIGN KEY (id) REFERENCES nodes(id)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "QUERY_WAYS = \"\"\"\n",
    "CREATE TABLE ways (\n",
    "    id INTEGER NOT NULL,\n",
    "    user TEXT,\n",
    "    uid INTEGER,\n",
    "    version INTEGER,\n",
    "    changeset INTEGER,\n",
    "    timestamp TEXT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "QUERY_WAYS_TAGS = \"\"\"\n",
    "CREATE TABLE ways_tags (\n",
    "    id INTEGER NOT NULL,\n",
    "    key TEXT NOT NULL,\n",
    "    value TEXT NOT NULL,\n",
    "    type TEXT,\n",
    "    FOREIGN KEY (id) REFERENCES ways(id)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "QUERY_WAYS_NODES = \"\"\"\n",
    "CREATE TABLE ways_nodes (\n",
    "    id INTEGER NOT NULL,\n",
    "    node_id INTEGER NOT NULL,\n",
    "    position INTEGER NOT NULL,\n",
    "    FOREIGN KEY (id) REFERENCES ways(id),\n",
    "    FOREIGN KEY (node_id) REFERENCES nodes(id)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "c.execute(QUERY_NODES)\n",
    "c.execute(QUERY_NODES_TAGS)\n",
    "c.execute(QUERY_WAYS)\n",
    "c.execute(QUERY_WAYS_TAGS)\n",
    "c.execute(QUERY_WAYS_NODES)\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('nodes.csv','rt') as fin:\n",
    "    dr = csv.DictReader(fin) # comma is default delimiter\n",
    "    to_db1 = [(i['id'], i['lat'], i['lon'], i['user'], i['uid'], i['version'], i['changeset'], i['timestamp']) for i in dr]\n",
    "    \n",
    "with open('nodes_tags.csv','rt') as fin:\n",
    "    dr = csv.DictReader(fin) # comma is default delimiter\n",
    "    to_db2 = [(i['id'], i['key'], i['value'], i['type']) for i in dr]\n",
    "    \n",
    "with open('ways.csv','rt') as fin:\n",
    "    dr = csv.DictReader(fin) # comma is default delimiter\n",
    "    to_db3 = [(i['id'], i['user'], i['uid'], i['version'], i['changeset'], i['timestamp']) for i in dr]\n",
    "    \n",
    "with open('ways_tags.csv','rt') as fin:\n",
    "    dr = csv.DictReader(fin) # comma is default delimiter\n",
    "    to_db4 = [(i['id'], i['key'], i['value'], i['type']) for i in dr]\n",
    "    \n",
    "with open('ways_nodes.csv','rt') as fin:\n",
    "    dr = csv.DictReader(fin) # comma is default delimiter\n",
    "    to_db5 = [(i['id'], i['node_id'], i['position']) for i in dr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.executemany(\"INSERT INTO nodes(id, lat, lon, user, uid, version, changeset, timestamp) VALUES (?, ?, ?, ?, ?, ?, ?, ?);\", to_db1)\n",
    "c.executemany(\"INSERT INTO nodes_tags(id, key, value, type) VALUES (?, ?, ?, ?);\", to_db2)\n",
    "c.executemany(\"INSERT INTO ways(id, user, uid, version, changeset, timestamp) VALUES (?, ?, ?, ?, ?, ?);\", to_db3)\n",
    "c.executemany(\"INSERT INTO ways_tags(id, key, value, type) VALUES (?, ?, ?, ?);\", to_db4)\n",
    "c.executemany(\"INSERT INTO ways_nodes(id, node_id, position) VALUES (?, ?, ?);\", to_db5)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify insertion into the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### At the beginning of the project, the osm file contained {'node': 226056, 'member': 19394, 'nd': 283605, 'tag': 116507, 'bounds': 1, 'note': 1, 'meta': 1, 'relation': 453, 'way': 32604, 'osm': 1}. Below is a SQL query that verifies that our database cooresponds to the original file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(226056,)]\n",
      "[(32604,)]\n"
     ]
    }
   ],
   "source": [
    "c.execute('SELECT COUNT(*) FROM nodes')\n",
    "all_rows = c.fetchall()\n",
    "print(all_rows)\n",
    "\n",
    "c.execute('SELECT COUNT(*) FROM ways')\n",
    "all_rows = c.fetchall()\n",
    "print(all_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the top 10 streets that contain the most nodes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Nashville Terminal Subdivision', 56), ('Rosa L Parks Boulevard', 45), ('Charlotte Avenue', 34), ('Four-Forty Parkway', 33), ('James Robertson Parkway', 31), ('4th Avenue South', 27), ('8th Avenue South', 25), ('West End Avenue', 23), ('Richland Creek Greenway', 21), ('Lafayette Street', 21)]\n"
     ]
    }
   ],
   "source": [
    "QUERY = '''\n",
    "SELECT ways_tags.value, COUNT(*)\n",
    "FROM ways_tags\n",
    "WHERE ways_tags.key = 'name'\n",
    "AND ways_tags.type = 'regular'\n",
    "GROUP BY ways_tags.value\n",
    "ORDER BY COUNT(*) DESC\n",
    "LIMIT 10;\n",
    "'''\n",
    "\n",
    "c.execute(QUERY)\n",
    "all_rows = c.fetchall()\n",
    "print(all_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the average number of nodes contained in a way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8.698472580051527,)]\n"
     ]
    }
   ],
   "source": [
    "QUERY = '''\n",
    "SELECT AVG(Count)\n",
    "FROM\n",
    "    (SELECT COUNT(*) as Count\n",
    "    FROM ways\n",
    "    JOIN ways_nodes\n",
    "    ON ways.id = ways_nodes.id\n",
    "    GROUP BY ways.id);\n",
    "'''\n",
    "\n",
    "c.execute(QUERY)\n",
    "all_rows = c.fetchall()\n",
    "print(all_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the top 10 amenities in Nashville?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('place_of_worship', 346), ('restaurant', 126), ('school', 79), ('bench', 60), ('fast_food', 52), ('bar', 51), ('cafe', 32), ('fuel', 25), ('parking_entrance', 21), ('parking', 17)]\n"
     ]
    }
   ],
   "source": [
    "QUERY = '''\n",
    "SELECT value, COUNT(*) as Count\n",
    "FROM nodes_tags\n",
    "WHERE key='amenity'\n",
    "GROUP BY value\n",
    "ORDER BY Count DESC\n",
    "LIMIT 10;\n",
    "'''\n",
    "\n",
    "c.execute(QUERY)\n",
    "all_rows = c.fetchall()\n",
    "print(all_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(226056,)]\n"
     ]
    }
   ],
   "source": [
    "QUERY = '''\n",
    "SELECT COUNT(*) FROM nodes;\n",
    "'''\n",
    "\n",
    "c.execute(QUERY)\n",
    "all_rows = c.fetchall()\n",
    "print(all_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(32604,)]\n"
     ]
    }
   ],
   "source": [
    "QUERY = '''\n",
    "SELECT COUNT(*) FROM ways;\n",
    "'''\n",
    "\n",
    "c.execute(QUERY)\n",
    "all_rows = c.fetchall()\n",
    "print(all_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Unique Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(877,)]\n"
     ]
    }
   ],
   "source": [
    "QUERY = '''\n",
    "SELECT COUNT(DISTINCT(e.uid))\n",
    "FROM (SELECT uid FROM nodes UNION ALL SELECT uid FROM ways) e;\n",
    "'''\n",
    "\n",
    "c.execute(QUERY)\n",
    "all_rows = c.fetchall()\n",
    "print(all_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 Contibutors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Gedwards724', 101230), ('wward', 44154), ('woodpeck_fixbot', 6743), ('ChesterKiwi', 5319), ('greggerm', 4813), ('JessAk71', 2840), ('Adam Martin', 2753), ('MikeGabrys', 2330), ('42429', 2276), ('Rub21', 2122)]\n"
     ]
    }
   ],
   "source": [
    "QUERY = '''\n",
    "SELECT e.user, COUNT(*) as num\n",
    "FROM (SELECT user FROM nodes UNION ALL SELECT user FROM ways) e\n",
    "GROUP BY e.user\n",
    "ORDER BY num DESC\n",
    "LIMIT 10;\n",
    "'''\n",
    "\n",
    "c.execute(QUERY)\n",
    "all_rows = c.fetchall()\n",
    "print(all_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review of Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I wrangled OpenStreetMap data for Nashville, TN as a json/osm file.\n",
    "- I cleaned the data of any problematic characters, resolved abbreviations, and other typographical errors in the data.\n",
    "- I used the provided data.py file to export the data into CSVs.\n",
    "- I used those csvs to create a SQL database.\n",
    "- I queried the data to answer questions about Nashville.\n",
    "- I provided an overview of the data and additional ideas for exploring the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use the data to identify popular areas for food and entertainment.\n",
    "- Additional cleaning could be done regarding zipcodes, other misplaced information, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py2_env]",
   "language": "python",
   "name": "conda-env-py2_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
